{"cells":[{"cell_type":"markdown","metadata":{"id":"0fkHK9OuU3ga"},"source":["# Homework 3: Generative Adversarial Networks\n","---\n","Here, you will study one of the most popular approaches to generative modeling. We will consider a toy problem and a simplified model since state-of-the-art generative models take weeks to converge on multiple GPUs. This will make the final results not that impressive, but still instructive.\n","\n","For this assignment, it is advised to use a **GPU** accelerator."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19245,"status":"ok","timestamp":1651336366111,"user":{"displayName":"Георгий М","userId":"03368392982034662548"},"user_tz":-180},"id":"cXJRyvMGU3gb","outputId":"80644d91-cd9c-4bc6-8555-4d1098002a6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘datasets’: File exists\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1LPYTu85QYYe_d1IS0l0v3fKzG3gjXrwC\n","To: /content/datasets/flowers-17.tar.gz\n","100% 1.71M/1.71M [00:00<00:00, 134MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1kpL8fGK2AkgCJmMcWklIuP8A8B8xCAC2\n","To: /content/part2_gans.tar.gz\n","100% 410k/410k [00:00<00:00, 109MB/s]\n","Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.6.2)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n","Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.2)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.3.0)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.2.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.8.1)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.8.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.8)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.44.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"]}],"source":["# Uncomment and run if in Colab\n","!mkdir datasets\n","!gdown --id 1LPYTu85QYYe_d1IS0l0v3fKzG3gjXrwC -O datasets/flowers-17.tar.gz\n","!tar -xzf datasets/flowers-17.tar.gz -C datasets\n","!rm datasets/flowers-17.tar.gz\n","!gdown --id 1kpL8fGK2AkgCJmMcWklIuP8A8B8xCAC2\n","!tar -xzf part2_gans.tar.gz\n","!rm part2_gans.tar.gz\n","\n","!pip install pytorch_lightning\n","!pip install scipy"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2040,"status":"ok","timestamp":1651336402747,"user":{"displayName":"Георгий М","userId":"03368392982034662548"},"user_tz":-180},"id":"_u6hHeduShS2","outputId":"39e435f8-9713-4f80-c8e2-d67c901f8a4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Determine the locations of auxiliary libraries and datasets.\n","# `AUX_DATA_ROOT` is where 'tiny-imagenet-2022.zip' is.\n","\n","# Detect if we are in Google Colaboratory\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","except ImportError:\n","    IN_COLAB = False\n","\n","from pathlib import Path\n","if IN_COLAB:\n","    google.colab.drive.mount(\"/content/drive\")\n","    \n","    # Change this if you created the shortcut in a different location\n","    AUX_DATA_ROOT = Path(\"/content/drive/MyDrive/DL_HW3\")\n","    \n","    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n","    \n","    import sys\n","    sys.path.append(str(AUX_DATA_ROOT))\n","    sys.path.append('/content/drive/MyDrive/DL_HW3/part2_gans')\n","else:\n","    AUX_DATA_ROOT = Path(\".\")"]},{"cell_type":"markdown","metadata":{"id":"1L2oUahMU3gb"},"source":["## Datasets\n","\n","We will use a pre-processed [17 category Flowers dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/17/). Below are few samples of the original images. We will use a processed version: images that are center square cropped and resized to 64 pixels.\n","\n","<img src=\"https://i.imgur.com/OYQd8JY.jpg\"/>"]},{"cell_type":"markdown","metadata":{"id":"ot4LVG7gU3gc"},"source":["## Assignments and grading\n","\n","\n","- **Part 1. Code**: fill in the empty gaps (marked with `#TODO`) in the code of the assignment (36 points):\n","    - `model.py` -- 26 points\n","    - `loss.py` -- 8 points\n","    - `train.py` -- 2 points\n","- **Part 2. Train and benchmark** the performance of the required models (7 points):\n","    - All 3 checkpoints are provided -- 3 points\n","    - All 4 variants are evaluated -- 4 points\n","- **Part 3. Report** your findings (9 points)\n","    - Each task -- 3 points\n","\n","- **Total score**: 50 points.\n","\n","The grading policy is the same as in the semantic segmentation task. It is provided below:\n","\n","For detailed grading of each coding assignment, please refer to the comments inside the files. Please use the materials provided during a seminar and during a lecture to do a coding part, as this will help you to further familiarize yourself with PyTorch. Copy-pasting the code from Google Search will get penalized.\n","\n","In part 2 of this task, you should upload all your pre-trained checkpoints to your personal Google Drive, grant public access and provide a file ID, following the intructions in the notebook.\n","\n","Note that for each task in part 3 to count towards your final grade, you should complete the corresponding tasks in part 2.\n","\n","For example, if you are asked to compare Model X and Model Y, you should provide the checkpoints for these models in your submission trained for the required number of epochs."]},{"cell_type":"markdown","metadata":{"id":"SWS5RaKgU3gc"},"source":["## Part 1. Code"]},{"cell_type":"markdown","metadata":{"id":"SUuvkN8TU3gc"},"source":["### `model.py`\n","**TODO: implement generator and discriminator models.**\n","\n","We will use DCGAN architecture as a base, but with a few key modifications which significantly improve the quality of the results.\n","\n","<img src=\"https://i.imgur.com/h4ubSt9.png\"/>\n","\n","#### 1. Each block is a pre-activation residual block.\n","\n","<img src=\"https://i.imgur.com/CqQM9mO.jpg\"/>\n","\n","It has a much better gradient flow compared to the standard residual block and is now fairly common in generative models. If needed, upsampling is performed at the start of the block (before branching), and downsampling is performed at the end (after residual sum).\n","\n","#### 2. We conditon on the noise vector multiple times throughout the network.\n","\n","One of the most popular ways of doing that is via adaptive batch normalization:\n","\n","$$\n","    x = \\frac{ x - \\mu }{ \\sigma } \\gamma + \\beta,\\quad \\gamma = f(z),\\ \\beta = g(z)\n","$$\n","\n","The first part of this operation is a standard batch normalization, but instead of optimizing $\\gamma$ and $\\beta$ as a vector, we optimize functions $f$ and $g$, which predict affine parameters from a noise vector $z$. Typically these functions are simple linear mappings.\n","\n","#### 3. We condition both generation and discrimination on classes.\n","\n","If our data is labeled with classes, we can use these to boost the performance of our GANs. The conditioning of the generator is straightforward: we simply train embeddings for each available class, and concatenate them with noise to use as inputs to the network and its adaptive batch normalization layers:\n","\n","<img src=\"https://i.imgur.com/VFiaU6N.jpg\"/>\n","\n","Therefore, our model produces its outputs using not only the noise vector $z$ but also on a class $k$, for each of which we train an embedding vector $c_k$.\n","\n","For the discriminator, one of the most popular ways of conditioning on a class label is by using the so-called \"projection\":\n","\n","<img src=\"https://i.imgur.com/jCwkb5R.png\"/>\n","\n","In this scheme, $\\phi$ denotes a convolutional part of the discriminator, which outputs a vector; $\\psi$ is a linear layer with maps a vector into a single digit; $y$ is a trainable class embedding. The output of this projection layer is fed into an adversarial loss. This layer allows the discriminator to learn whether or not a synthesized image belongs to the class which we input into the generator.\n","\n","To sum up, generator class embeddings $c$ and discriminator embeddings $y$ are vectors from trainable matrices of the shape $\\text{number of classes} \\times \\text{dimensionality of the embeddings}$, corresponding to the class $k$ which we condition our sample on. In our case, these matrices will be different for generator and discriminator and will have different embedding dimensionalities."]},{"cell_type":"markdown","metadata":{"id":"xTiDEnyhU3gd"},"source":["### `loss.py`\n","**TODO: implement train and validation losses.**\n","\n","#### Training\n","\n","There are multiple ways to train generative adversarial networks. We will try out 3 of them, which historically preceded each other.\n","\n","#### 1. Non-saturating GAN\n","\n","$$\n","    \\mathcal{L}_D = - \\mathbb{E}_{x\\sim p_\\text{real}} [ \\log D(x) ] - \\mathbb{E}_{z\\sim \\mathcal{N}(0, \\mathbb{I})} [ \\log(1 - D(G(z)) ]\n","$$\n","\n","$$\n","    \\mathcal{L}_G = - \\mathbb{E}_{z\\sim \\mathcal{N}(0, \\mathbb{I})} [\\log D(G(z))]\n","$$\n","\n","It corresponds to using a standard binary cross-entropy loss for $D$, and BCE with fake data treated as real data for $G$.\n","\n","#### 2. Hinge Loss GAN\n","\n","$$\n","    \\mathcal{L}_D = -\\mathbb{E}_{x\\sim p_\\text{real}} \\big[ \\min\\big(0, -1 + D(x) \\big) \\big] - \\mathbb{E}_{z\\sim \\mathcal{N}(0, \\mathbb{I})} \\big[ \\min\\big(0, -1 - D(G(z)) \\big) \\big]\n","$$\n","\n","$$\n","    \\mathcal{L}_G = - \\mathbb{E}_{z\\sim \\mathcal{N}(0, \\mathbb{I})} D(G(z))\n","$$\n","\n","This objective is derived from a hinge loss (used, for example, as an objective in SVMs). Arguably, it has the best gradient flow, and now it a go-to objective for GAN training.\n","\n","#### Validation\n","\n","For validation, we will use two main metrics: **Frechet Inception Distance (FID)** and **Inception Score (IS)**.\n","\n","They are both calculated using the outputs of an **Inception v3** network (hence \"inception\" in their names), although any other pre-trained classification network can also be used in the same way to obtain similar metrics.\n","\n","#### 1. Frechet Inception Distance\n","\n","This metric is calculated using a feature vector right after global average pooling before the final classification head. The feature vector can be treated as a multi-dimensional random variable with some distribution. This distribution will be different, if we evaluate these features using real images from the dataset, or images generated using our generative models. The general idea behind **FID** is to try and approximate the difference between these two distributions and use it as a quality metric (the lower it is, the better).\n","\n","To do that, we approximate these two distribution using a multivariate gaussian distribution. To do that, we need to calculate the mean vector $\\mu$ and a covariance matrix $\\Sigma$ using either samples from the dataset: $\\mu_r$, $\\Sigma_r$, or generated samples: $\\mu_g$, $\\Sigma_g$. Note that these are full covariance matrices.\n","\n","Then, **FID** can be calculated using KL divergence between these two distributions:\n","\n","$$\n","    \\text{FID} = ||\\mu_r - \\mu_g||^2 + \\text{tr}\\,\\big(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}\\big)\n","$$\n","\n","#### 2. Inception Score\n","\n","For this metric, we will need the outputs of the classification head, which we should convert to class probabilities via a softmax. \n","\n","To calculate it, we will only use generated data, and try to evaluate two qualities: their \"objectiveness\", and the diversity.\n","\n","For the \"objectiveness\" metric, we can look at the distribution of the class probabilities and check whether or not it has a pike. Here, we assume that outputs of our generative models should represent objects, which have a structure, similar to an ImageNet dataset. This would be a bad assumption if we generate X-ray or other medical images, but it's actually fairly true for natural images, thanks to the diversity of ImageNet. If our model generates a smeared blob of artifacts, it is unlikely to be classified as some object by an ImageNet classifier.\n","\n","A good measure to determine if the distribution is \"piky\" is entropy. It is the lowest if predicted probability is a one-hot vector, and highest if it is uniform accross all calsses.\n","\n","For \"diversity\", we are going to use the same idea: our samples are diverse, if their averaged class probability distribution is uniform.\n","\n","Combining these two measurements, we can come up with the following objective:\n","\n","$$\n","    \\text{IS} = \\exp \\Bigg[ \\mathbb{E}_{\\hat{x}\\sim p(\\hat{x})}\\ \\text{KL} \\big( p(y \\mid \\hat{x})\\ \\big\\|\\ p(y) \\big) \\Bigg] = \\exp \\Bigg[ \\mathbb{E}_{\\hat{x}\\sim p(\\hat{x})} \\sum_{k=1}^{K}\\ p(y_k \\mid \\hat{x}) \\log \\bigg[ \\frac{ p(y_k \\mid \\hat{x} ) }{ p(y_k) } \\bigg] \\Bigg]\n","$$\n","\n","where $K = 1000$ for ImageNet-pretrained networks.\n","\n","For more details about derivation and applicability, you can refer to this [link](https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a)."]},{"cell_type":"markdown","metadata":{"id":"-y_DMkdoU3gh"},"source":["### `train.py`\n","\n","Here you will need to write a training step for GANs (alternating gradients descend, where we first update the generator, and thenn a discriminator), and also implement a neat feature called \"truncation trick\".\n","\n","There are multiple ways to improve test-time performance of trained GANs (i.e., obtain better samples). Some are more complicated, like [usage of Langevin dynamics](https://arxiv.org/abs/2003.06060) for sampling, some are much simpler, like [rejection sampling](https://arxiv.org/abs/1810.06758). We will consider the simplest, yet one of the most effective and universally used approaches: [truncation trick](https://paperswithcode.com/method/truncation-trick).\n","\n","The idea is based on an observation that if, instead of $\\mathcal{N}(0, \\mathbb{I})$, we sample from a truncated normal distribution, the results that we get will have a better visual quality. You will have to implement sampling from a truncated normal distribution and use it during evaluation."]},{"cell_type":"markdown","metadata":{"id":"dBCf3YeUU3gh"},"source":["## Part 2. Train and evaluate\n","\n","You will have to train and evaluate the following variants for the generative model:\n","\n","1. Non-class conditional setting: non-saturating GAN and hinge Loss GAN\n","2. Class conditional hinge loss GAN\n","3. Evaluate class conditional hinge loss GAN with truncation trick\n","\n","For training, use the code example below, with the provided number of epochs. For evaluation use `GANValLoss` class that you have implemented. You need to obtain **FID** and **IS** values for all the 4 required experiments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9459VHzBXTo"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHs08EHAU3gi"},"outputs":[],"source":["import pytorch_lightning as pl\n","from part2_gans.train import GAN\n","\n","\n","\n","def train(model, experiment_name, use_gpu):\n","    assignment_dir = 'part2_gans'\n","\n","    logger = pl.loggers.TensorBoardLogger(save_dir=f'{assignment_dir}/logs', name=experiment_name)\n","\n","    trainer = pl.Trainer(\n","        max_epochs=150, \n","        gpus=1 if use_gpu else None, \n","        benchmark=True, \n","        logger=logger) \n","    \n","    trainer.fit(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_Pb9DZUBXTp"},"outputs":[],"source":["use_gpu = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642,"referenced_widgets":["3e2375acdd0145d39c014251f1625d88","e980ba19f0f2431f81256223ace138f1","9d88c87a1833415fae1b11011d519fff","928c47179c7649cda85c0d08d7d941f0","234a3fff34ab4b7e87f8f3e3354c2530","4c835e752ed440b098e6663bce2fe5f8","5af8511174c4458f92d09a51ee696a2a","fdec8cc1cab742a59b01eede1f7eadc8","14ababdffe33431ba9d167c1d132a036","37d3e7515d1a4986afe43a4f07dd7134","af5648403ff14fa0af9af4de5e1fa7f3"]},"executionInfo":{"elapsed":5556,"status":"ok","timestamp":1651333029683,"user":{"displayName":"Георгий М","userId":"03368392982034662548"},"user_tz":-180},"id":"A30Ph3FqU3gi","scrolled":false,"outputId":"cb49b87d-8799-4792-ca0f-98c5815133af"},"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n","  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:308: LightningDeprecationWarning: The `LightningModule.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_end` instead.\n","  f\"The `LightningModule.{hook}` hook was deprecated in v1.6 and\"\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name | Type          | Params\n","---------------------------------------\n","0 | gen  | Generator     | 3.6 M \n","1 | dis  | Discriminator | 4.9 M \n","2 | loss | GANLoss       | 0     \n","---------------------------------------\n","8.5 M     Trainable params\n","0         Non-trainable params\n","8.5 M     Total params\n","33.885    Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1931: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  category=PossibleUserWarning,\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2375acdd0145d39c014251f1625d88"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n","  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"]}],"source":["import torch\n","\n","model = GAN(\n","    loss_type='non_saturating',\n","    class_conditional=False,\n","    truncation_trick=False, \n","    data_path='datasets/flowers-17')\n","\n","train(model, 'non_saturating', use_gpu=use_gpu)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"referenced_widgets":["a3b279dabc0644338bf147178e9c91a8","3fb931ca6c304ccaaa274c840a14325d","f9c9d4e2faa548d6a47995de75f4f9ef","0e3f1a84a07e4e979f7af693a7ef2900","a795452338fc4a35913ad0d8d7fc1084","6d498f05128f471a893d3a1ede5fcd89","64ed1054816b48889ab5de6e3426d692","dee7f3f5aba94d3d9563cdc5e70cf620","9365c6263dbd4abea8bb8c82fa5e8c04","bd0e0bf985c54b0a92f5a51ae6a828dd","1c706a66b39640018717850b693b6aab"]},"id":"6yyr5AntBXTq","executionInfo":{"status":"ok","timestamp":1651312314968,"user_tz":-180,"elapsed":667840,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"128c89ee-e184-48ea-e948-464fee37b388"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n","  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:308: LightningDeprecationWarning: The `LightningModule.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_end` instead.\n","  f\"The `LightningModule.{hook}` hook was deprecated in v1.6 and\"\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name | Type          | Params\n","---------------------------------------\n","0 | gen  | Generator     | 3.6 M \n","1 | dis  | Discriminator | 4.9 M \n","2 | loss | GANLoss       | 0     \n","---------------------------------------\n","8.5 M     Trainable params\n","0         Non-trainable params\n","8.5 M     Total params\n","33.885    Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1931: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  category=PossibleUserWarning,\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3b279dabc0644338bf147178e9c91a8","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = GAN(\n","    loss_type='hinge',\n","    class_conditional=False,\n","    truncation_trick=False, \n","    data_path='datasets/flowers-17')\n","\n","train(model, 'hinge', use_gpu=use_gpu)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"referenced_widgets":["5700e4ce557842db896bd13d23d3dcc7","7b7e327562d34484b8179ba66ed4ae30","4e92d79266724655a70215a85cdfc874","71c047db424544a5b0d36b9222143088","18e7ba787a9d4f01842e8364614fd9d7","95b02f5205d7403c90f6a4ddf36477e5","3a263215db5d41dc9c0a28517515e90e","3042203df9644683beff26d941224190","d8a4749d9d4749bcb25fff3a9b2e2bc4","2744ed8114f6419d8720634de58012b7","6e2d7b75881e468bb41826d8b18622b3"]},"id":"U9Akkl_eBXTr","executionInfo":{"status":"ok","timestamp":1651329634057,"user_tz":-180,"elapsed":1740850,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"6cf97e41-88be-4160-c873-2186c867d36d"},"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n","  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:308: LightningDeprecationWarning: The `LightningModule.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_end` instead.\n","  f\"The `LightningModule.{hook}` hook was deprecated in v1.6 and\"\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name | Type          | Params\n","---------------------------------------\n","0 | gen  | Generator     | 5.4 M \n","1 | dis  | Discriminator | 4.9 M \n","2 | loss | GANLoss       | 0     \n","---------------------------------------\n","10.3 M    Trainable params\n","0         Non-trainable params\n","10.3 M    Total params\n","41.040    Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1931: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  category=PossibleUserWarning,\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5700e4ce557842db896bd13d23d3dcc7"}},"metadata":{}}],"source":["model = GAN(\n","    loss_type='hinge',\n","    class_conditional=True,\n","    truncation_trick=False, \n","    data_path='datasets/flowers-17')\n","\n","train(model, 'hinge_class-cond', use_gpu=use_gpu)"]},{"cell_type":"code","source":["model = GAN(\n","    loss_type='hinge',\n","    class_conditional=True,\n","    truncation_trick=True, \n","    data_path='datasets/flowers-17')\n","\n","train(model, 'hinge_class-cond-trunc', use_gpu=use_gpu)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":620,"referenced_widgets":["46321199fc5c4a1b8a752a3035b4d67d","83110a847b7a4216b0d12e68f3e9ee5a","40e8efe3229d4ea9bf25d556d9b7b760","cabf177a0168425493eedc2da4c15c55","8da6eda53a664bb0915fe9d7951b0021","7b280672e052496c9a8c183f81548fa5","fce726b947c041fa845b9cc9682c1986","ff3df68f65b2418cb1b4fecd87d3cee5","65ef6759dc364792a317103355d1d82b","79d4fc3bdce44b23a4c1774dbbe8cd78","ebeb5ce7d3d84b5dbd771c715000e932"]},"id":"B306EKe7BHlZ","executionInfo":{"status":"ok","timestamp":1651332694525,"user_tz":-180,"elapsed":1057019,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"3d4916d2-cd9e-4127-a01d-3246c8fd6f28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n","  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:308: LightningDeprecationWarning: The `LightningModule.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_end` instead.\n","  f\"The `LightningModule.{hook}` hook was deprecated in v1.6 and\"\n","Missing logger folder: part2_gans/logs/hinge_class-cond-trunc\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name | Type          | Params\n","---------------------------------------\n","0 | gen  | Generator     | 5.4 M \n","1 | dis  | Discriminator | 4.9 M \n","2 | loss | GANLoss       | 0     \n","---------------------------------------\n","10.3 M    Trainable params\n","0         Non-trainable params\n","10.3 M    Total params\n","41.040    Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1931: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  category=PossibleUserWarning,\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46321199fc5c4a1b8a752a3035b4d67d"}},"metadata":{}}]},{"cell_type":"code","execution_count":40,"metadata":{"id":"NM6D4-jiTMYu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651336504529,"user_tz":-180,"elapsed":783,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"1fe83330-8de4-46a2-93dc-0954f150bc20"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat 'logs.zip': No such file or directory\n","checkpoints\thw3-GANs-Mkrtchyan-Georgy-attempt-1.ipynb  part2_gans\n","flowers-17.tar\tlogs.zip\n"]}],"source":["# Copy log folder to Google Drive\n","!cp logs.zip '{AUX_DATA_ROOT}/logs.zip'\n","!ls '{AUX_DATA_ROOT}'"]},{"cell_type":"markdown","metadata":{"id":"7ywuVgq1U3gi"},"source":["Again, images can be viewed via a TensorBoard:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfpY3tLDU3gj"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir part2_gans/logs"]},{"cell_type":"markdown","metadata":{"id":"hCcEyBLyBXTr"},"source":["Your trained weights are available in the `part2_gans/{experiment_name}/logs/version_{n}` folder. Upload them to your personal Google Drive folder. Provide file ids and checksums below. Use `!md5sum <PATH>` to compute the checksums.\n","\n","To make sure that provided ids are correct, try running `!gdown --id <ID>` command from this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8M6ujty9BXTs"},"outputs":[],"source":["checkpoint_ids = {\n","    'non_saturating': ('1-ylh_jcz8ZcWjz_s_8yATc8FFnGgF9Qb', '0b3610881c9a5f2ad47d1240ec960d3b'), # TODO\n","    'hinge': ('1ZU1a3Txxwi_TmEpp4CNZ7yifIpNuThEF','591483058c1c6bbcf2df021a8e3b98cc'), # TODO\n","    'hinge_class-cond': ('1fXlB_yyb7k1oagOkOSJviUWvXB1NuoFF','9e663e52651b5429bbf62261e17a14cf'),\n","    'hinge_class-cond-trunc':('1fXlB_yyb7k1oagOkOSJviUWvXB1NuoFF','d14f50a16d65e1573d0c977947c98fa6') # TODO\n","}"]},{"cell_type":"markdown","metadata":{"id":"SPuKis16U3gj"},"source":["**FID** and **IS** can be calculated like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yKgug92U3gj"},"outputs":[],"source":["import torch\n","from torchvision import utils, transforms\n","import glob\n","import os\n","\n","from part2_gans.loss import ValLoss\n","\n","\n","\n","def load_checkpoint(model, experiment_name):\n","    version = max([int(name.split('_')[-1]) for name in os.listdir(f'part2_gans/logs/{experiment_name}')])\n","    path_to_checkpoint = glob.glob(f'part2_gans/logs/{experiment_name}/version_{version}/checkpoints/*.ckpt')[0]\n","    model.load_state_dict(torch.load(path_to_checkpoint)['state_dict'], strict=False)\n","\n","def calc_eval_metrics(model, device):\n","    dataloader = model.val_dataloader()\n","    \n","    if device == 'cuda':\n","        model = model.cuda()\n","    \n","    val_noise = model.val_noise\n","\n","    noise_offset = 0\n","    \n","    with torch.no_grad():\n","        real_imgs = []\n","        fake_imgs = []\n","\n","        for imgs, labels in dataloader:\n","            noise = val_noise[noise_offset : noise_offset + imgs.shape[0]]\n","            noise_offset += imgs.shape[0]\n","\n","            if device == 'cuda':\n","                imgs = imgs.cuda()\n","                labels = labels.cuda()\n","                noise = noise.cuda()\n","\n","            gen_imgs = model.forward(noise, labels)\n","\n","            real_imgs.append(imgs)\n","            fake_imgs.append(gen_imgs)\n","\n","        val_loss = ValLoss()\n","\n","        if device == 'cuda':\n","            val_loss = val_loss.cuda()\n","\n","        fid, inception_score = val_loss(real_imgs, fake_imgs)\n","    \n","    return fid, inception_score\n","\n","def visualize_image_grid(model):\n","    noise = model.val_noise[:16 * model.num_classes]\n","    labels = torch.arange(model.num_classes).repeat_interleave(16, dim=0).to(noise.device)\n","\n","    fake_imgs = model.forward(noise, labels)\n","    fake_imgs = fake_imgs.detach().cpu()\n","\n","    grid = utils.make_grid(fake_imgs, nrow=16)\n","    \n","    return transforms.ToPILImage()(grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xCxoPYMBXTt","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1a213Y2mjzZ2Qk7nbMBZodaZ4nRW7qufJ"},"executionInfo":{"status":"ok","timestamp":1651333186715,"user_tz":-180,"elapsed":16273,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"bfd87d57-4712-4bad-c346-ff667b69e9d7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model = GAN(\n","    loss_type='non_saturating',\n","    class_conditional=False,\n","    truncation_trick=False, \n","    data_path='datasets/flowers-17')\n","load_checkpoint(model, 'non_saturating')\n","\n","fid, inception_score = calc_eval_metrics(model.eval().cuda(), 'cuda' if use_gpu else 'cpu')\n","\n","print(f'FID: {fid:.2f}, IS: {inception_score:.2f}')\n","visualize_image_grid(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbREfC2zBXTt","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1QhA-qyZb-dBDNkEMuQnDX3Ets9RkHdMH"},"executionInfo":{"status":"ok","timestamp":1651334211088,"user_tz":-180,"elapsed":20132,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"b6c17930-c067-47a9-f9e0-73ada1f9a81a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model = GAN(\n","    loss_type='hinge',\n","    class_conditional=False,\n","    truncation_trick=False, \n","    data_path='datasets/flowers-17')\n","load_checkpoint(model, 'hinge')\n","\n","fid, inception_score = calc_eval_metrics(model.eval().cuda(), 'cuda' if use_gpu else 'cpu')\n","\n","print(f'FID: {fid:.2f}, IS: {inception_score:.2f}')\n","visualize_image_grid(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtnSZ8kyBXTt","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wVBMxPwFib0mgEmVOkzFrf0BKhjPFyUb"},"executionInfo":{"status":"ok","timestamp":1651333538764,"user_tz":-180,"elapsed":16480,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"998bee55-ed94-4680-b004-4143db53ef86"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model = GAN(\n","    loss_type='hinge',\n","    class_conditional=True,\n","    truncation_trick=False, \n","    data_path='datasets/flowers-17')\n","load_checkpoint(model, 'hinge_class-cond')\n","\n","fid, inception_score = calc_eval_metrics(model.eval().cuda(), 'cuda' if use_gpu else 'cpu')\n","\n","print(f'FID: {fid:.2f}, IS: {inception_score:.2f}')\n","visualize_image_grid(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSq038UlBXTt","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1eDfoASt3zAeCsZiIIFKsFy_iGhta0kcR"},"executionInfo":{"status":"ok","timestamp":1651332907412,"user_tz":-180,"elapsed":14499,"user":{"displayName":"Георгий М","userId":"03368392982034662548"}},"outputId":"f5c54251-dbba-4ff9-8b25-76ddfea8c6c4"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model = GAN(\n","    loss_type='hinge',\n","    class_conditional=True,\n","    truncation_trick=True, \n","    data_path='datasets/flowers-17')\n","load_checkpoint(model, 'hinge_class-cond-trunc')\n","\n","fid, inception_score = calc_eval_metrics(model.eval().cuda(), 'cuda' if use_gpu else 'cpu')\n","\n","print(f'FID: {fid:.2f}, IS: {inception_score:.2f}')\n","\n","visualize_image_grid(model)"]},{"cell_type":"markdown","metadata":{"id":"zfYTf9xSU3gj"},"source":["## Part 3. Report\n","\n","In this part, you will need to analyze and compare the quality and performance of the trained models. Like semantic segmentation homework."]},{"cell_type":"markdown","metadata":{"id":"8tjZyz1wU3gj"},"source":["### Task 1.\n","\n","Compare the performance of two evaluated GAN losses both qualitatively (comparing generated images side-by-side) and quantitatively (via metrics). What objective leads to the best results?\n","\n","\n"]},{"cell_type":"markdown","source":["Unlike the earlier inception score (IS), which evaluates only the distribution of generated images, the FID compares the distribution of generated images with the distribution of real images that were used to train the generator."],"metadata":{"id":"ZyybeToD4z3X"}},{"cell_type":"markdown","source":["Model|FID | IS\n","--- | --- | ---\n","non-staturating|170.76 | 2.43\n","hinge|166.62| 3.03\n","hinge class conditioned|122.91| 3.19 \n","hinge class conditioned trunc trick|133.29| 3.95\n","\n","Generally, we can observe that all variations of the hinge loss outperform non-saturating loss. However let us now consider only non-saturating loss and simple hinge loss without class conditioning and truncation tricks to have equal conditions for comparison.\n","\n","\n","\n","Let us take a look on the images:\n"],"metadata":{"id":"fVgFoB7zKrxr"}},{"cell_type":"markdown","source":["non_saturating             |  hinge\n",":-------------------------:|:-------------------------:\n","![](https://drive.google.com/uc?export=view&id=11O1z3ZlirIVeYbhCLKPlgO0F3rh5P7DW)|![](https://drive.google.com/uc?export=view&id=1wEXCuAhaTjNo8gm2-JoFn0ZfoscXDH0w)\n","\n"],"metadata":{"id":"JMj9SjlEXzdg"}},{"cell_type":"markdown","source":["non_saturating             |  hinge\n",":-------------------------:|:-------------------------:\n","![](https://drive.google.com/uc?export=view&id=1cqmg0cXsKq5Agd4CQk8ECGdxO0QXNkXt)|![](https://drive.google.com/uc?export=view&id=143vfmU1pYBPqccsimdtfRDiLxjdhHAQA)"],"metadata":{"id":"4j7EkkEPY7qk"}},{"cell_type":"markdown","source":["First subsample of images is associated with non-saturating loss.As we can see those images looks more like Van Gogh paintings , more precisely, they are poorly structured in comparison to hinge loss. Moreover, hinge loss produces better shapes than non-saturating loss. In addition to those advantages of hinge loss we can also state that , pictures of the hinge loss GAN are more diversed, i.e. as can be seen from the pictur non saturating loss produces images from similar color range - mosstly used Yellow, Green , White, and also flowers are seems to be same - set of 2-3 species. However, hinge loss was capable of capturing different species of flowers - at least 5-6 on the presented picture and also it uses in addition purple and red colors.\n","Thus given our architecture hinge loss outperforms non-saturating one."],"metadata":{"id":"rUS7NcbROZuy"}},{"cell_type":"markdown","metadata":{"id":"0gPoboKtU3gk"},"source":["### Task 2.\n","Compare (qualitatively and quantitatively) class conditional and non-class conditional models. Which one has better quality and metrics? Reflect and propose an explanation, why is that so?"]},{"cell_type":"markdown","source":["Firstly, we can see from the table above that class conditioned hinge loss GAN has better metrics - FID HL = 166.62 vs FID CC HL = 122.91 and IS HL = 3.03 vs IS CC HL = 3.19, i.e CC HL GAN has better qulaity of generated images not only independently of real images (IS) but also in comparison to distribution of real image (FID)."],"metadata":{"id":"9CKj_MgTbNne"}},{"cell_type":"markdown","source":["hinge             |  hinge class-conditioned\n",":-------------------------:|:-------------------------:\n","![](https://drive.google.com/uc?export=view&id=17hjennF2_M8NXIqrDiKGPm_wsdg-o2nh)|![](https://drive.google.com/uc?export=view&id=18BAlgZ9zqZuOExK39fglnSYJ1Gb35q8b)\n"],"metadata":{"id":"WB0mb_bbQYvy"}},{"cell_type":"markdown","source":["hinge             |  hinge class-conditioned\n",":-------------------------:|:-------------------------:\n","![](https://drive.google.com/uc?export=view&id=143vfmU1pYBPqccsimdtfRDiLxjdhHAQA)|![](https://drive.google.com/uc?export=view&id=1f07AzaA2ujId90m11v-6tqKtweeQLaLG)\n"],"metadata":{"id":"Ts7gdR3QQY22"}},{"cell_type":"markdown","source":["Talking about the quality of the images. CC Conditioinal hinge loss gives a bit better quality then  simple hinge loss , since additional information in form of labels is provided. But generally both approaches have similar quality of the images. But the most important difference is that CC HL GAN was subject to mode collapse,i.e it produces same pictures for the same label, such a phenomenon appears due to the fact that generator withdraw good image for some observation, and thus it uses it for all other images. Such a problem may be solved via some regularization or use of different loss , one of the main suggestions in the community is to use Wasserstein loss( No adjustments were produced since grading requires exact implementation of the provided losses)"],"metadata":{"id":"ERkWQVfqa4uc"}},{"cell_type":"markdown","metadata":{"id":"NHrDMhB2U3gk"},"source":["### Task 3.\n","Do the same comparison with and without truncation trick. Explain, what changes when this trick is applied, how it affects the results and their quality? Try to explain, why exactly truncation trick works this way?"]},{"cell_type":"markdown","source":["*Quantitative comparison:*\n","\n","FID CC HL = 122.91 vs FID CC HL with truncation  = 133.29 and FID CC HL = 3.19 vs FID CC HL with truncation = 3.95\n","\n","As we can see truncation trick reduces performance of the GAN in comparison to the distribution of real images(FID higher) , however, it imporves the performance of the model in terms of own distribution(IS higer)\n","Such an issue may be described by the fact that we withdraw noise not from the whole range of distribution but only from some part which decreases the variability of the own distribution of the model. However, such an operation may have decreased the ability of the generator to learn crucial cases of images, and thus it has lower performance in comparison to the real images distribution.\n","\n","*Qualitative comparison:*\n","Genarally, the images of both models are pretty similar, there is no severe distinction between them.However, some pictures of model with truncation trick are messy and a bit less structured, whereas another are better structured. \n","To sum up , there is no severe observable (at least for my eyes) distinction between the results of both  models"],"metadata":{"id":"8EEVvOuzeYDW"}},{"cell_type":"markdown","source":["hinge class-conditioned      |  hinge class-conditioned with truncation\n",":-------------------------:|:-------------------------:\n","![](https://drive.google.com/uc?export=view&id=18BAlgZ9zqZuOExK39fglnSYJ1Gb35q8b)|![](https://drive.google.com/uc?export=view&id=1mf4dS3VgD4OFHk_rlhZDIYhIWv1DOn-X)\n"],"metadata":{"id":"9kPW30pMN3p7"}},{"cell_type":"markdown","source":["hinge class-conditioned      |  hinge class-conditioned with truncation\n",":-------------------------:|:-------------------------:\n","![](https://drive.google.com/uc?export=view&id=1f07AzaA2ujId90m11v-6tqKtweeQLaLG)|![](https://drive.google.com/uc?export=view&id=1jmolL_6mr0RVcZsEK4Nrur03NbHdiL3I)\n"],"metadata":{"id":"X69eFAwEN3_Q"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"hw3-GANs-Mkrtchyan-Georgy-attempt-1.ipynb","provenance":[{"file_id":"197xfHoPrJD97XQbyPJXCZij7vbtrk77s","timestamp":1650725491747}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3e2375acdd0145d39c014251f1625d88":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e980ba19f0f2431f81256223ace138f1","IPY_MODEL_9d88c87a1833415fae1b11011d519fff","IPY_MODEL_928c47179c7649cda85c0d08d7d941f0"],"layout":"IPY_MODEL_234a3fff34ab4b7e87f8f3e3354c2530"}},"e980ba19f0f2431f81256223ace138f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c835e752ed440b098e6663bce2fe5f8","placeholder":"​","style":"IPY_MODEL_5af8511174c4458f92d09a51ee696a2a","value":"Epoch 0: 100%"}},"9d88c87a1833415fae1b11011d519fff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdec8cc1cab742a59b01eede1f7eadc8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14ababdffe33431ba9d167c1d132a036","value":1}},"928c47179c7649cda85c0d08d7d941f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37d3e7515d1a4986afe43a4f07dd7134","placeholder":"​","style":"IPY_MODEL_af5648403ff14fa0af9af4de5e1fa7f3","value":" 22/22 [00:13&lt;00:00,  1.59it/s, loss=1.83, v_num=1, loss_gen=1.530, loss_dis=0.665]"}},"234a3fff34ab4b7e87f8f3e3354c2530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"4c835e752ed440b098e6663bce2fe5f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5af8511174c4458f92d09a51ee696a2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdec8cc1cab742a59b01eede1f7eadc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14ababdffe33431ba9d167c1d132a036":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37d3e7515d1a4986afe43a4f07dd7134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af5648403ff14fa0af9af4de5e1fa7f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3b279dabc0644338bf147178e9c91a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fb931ca6c304ccaaa274c840a14325d","IPY_MODEL_f9c9d4e2faa548d6a47995de75f4f9ef","IPY_MODEL_0e3f1a84a07e4e979f7af693a7ef2900"],"layout":"IPY_MODEL_a795452338fc4a35913ad0d8d7fc1084"}},"3fb931ca6c304ccaaa274c840a14325d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d498f05128f471a893d3a1ede5fcd89","placeholder":"​","style":"IPY_MODEL_64ed1054816b48889ab5de6e3426d692","value":"Epoch 599: 100%"}},"f9c9d4e2faa548d6a47995de75f4f9ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dee7f3f5aba94d3d9563cdc5e70cf620","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9365c6263dbd4abea8bb8c82fa5e8c04","value":1}},"0e3f1a84a07e4e979f7af693a7ef2900":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd0e0bf985c54b0a92f5a51ae6a828dd","placeholder":"​","style":"IPY_MODEL_1c706a66b39640018717850b693b6aab","value":" 22/22 [59:02&lt;00:00, 161.01s/it, loss=2.9, v_num=1, loss_gen=6.140, loss_dis=0.370]"}},"a795452338fc4a35913ad0d8d7fc1084":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6d498f05128f471a893d3a1ede5fcd89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64ed1054816b48889ab5de6e3426d692":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dee7f3f5aba94d3d9563cdc5e70cf620":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9365c6263dbd4abea8bb8c82fa5e8c04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd0e0bf985c54b0a92f5a51ae6a828dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c706a66b39640018717850b693b6aab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5700e4ce557842db896bd13d23d3dcc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b7e327562d34484b8179ba66ed4ae30","IPY_MODEL_4e92d79266724655a70215a85cdfc874","IPY_MODEL_71c047db424544a5b0d36b9222143088"],"layout":"IPY_MODEL_18e7ba787a9d4f01842e8364614fd9d7"}},"7b7e327562d34484b8179ba66ed4ae30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95b02f5205d7403c90f6a4ddf36477e5","placeholder":"​","style":"IPY_MODEL_3a263215db5d41dc9c0a28517515e90e","value":"Epoch 249: 100%"}},"4e92d79266724655a70215a85cdfc874":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3042203df9644683beff26d941224190","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8a4749d9d4749bcb25fff3a9b2e2bc4","value":1}},"71c047db424544a5b0d36b9222143088":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2744ed8114f6419d8720634de58012b7","placeholder":"​","style":"IPY_MODEL_6e2d7b75881e468bb41826d8b18622b3","value":" 22/22 [28:59&lt;00:00, 79.06s/it, loss=2.03, v_num=3, loss_gen=2.210, loss_dis=0.862]"}},"18e7ba787a9d4f01842e8364614fd9d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"95b02f5205d7403c90f6a4ddf36477e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a263215db5d41dc9c0a28517515e90e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3042203df9644683beff26d941224190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a4749d9d4749bcb25fff3a9b2e2bc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2744ed8114f6419d8720634de58012b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e2d7b75881e468bb41826d8b18622b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46321199fc5c4a1b8a752a3035b4d67d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83110a847b7a4216b0d12e68f3e9ee5a","IPY_MODEL_40e8efe3229d4ea9bf25d556d9b7b760","IPY_MODEL_cabf177a0168425493eedc2da4c15c55"],"layout":"IPY_MODEL_8da6eda53a664bb0915fe9d7951b0021"}},"83110a847b7a4216b0d12e68f3e9ee5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b280672e052496c9a8c183f81548fa5","placeholder":"​","style":"IPY_MODEL_fce726b947c041fa845b9cc9682c1986","value":"Epoch 149: 100%"}},"40e8efe3229d4ea9bf25d556d9b7b760":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff3df68f65b2418cb1b4fecd87d3cee5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65ef6759dc364792a317103355d1d82b","value":1}},"cabf177a0168425493eedc2da4c15c55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79d4fc3bdce44b23a4c1774dbbe8cd78","placeholder":"​","style":"IPY_MODEL_ebeb5ce7d3d84b5dbd771c715000e932","value":" 22/22 [17:34&lt;00:00, 47.92s/it, loss=2.16, v_num=0, loss_gen=3.250, loss_dis=0.482]"}},"8da6eda53a664bb0915fe9d7951b0021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"7b280672e052496c9a8c183f81548fa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fce726b947c041fa845b9cc9682c1986":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff3df68f65b2418cb1b4fecd87d3cee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65ef6759dc364792a317103355d1d82b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79d4fc3bdce44b23a4c1774dbbe8cd78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebeb5ce7d3d84b5dbd771c715000e932":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}